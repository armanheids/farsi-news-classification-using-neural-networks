# -*- coding: utf-8 -*-
"""News_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NXcKscKXGEmSNh6cEkJ0XKpvzVcAm9d

## Install HAZM Library
"""

!pip install hazm

"""## Import Essential Library"""

import numpy as np
import pandas as pd

"""## Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""## Unzip Dataset"""

!unzip -xq '/content/drive/MyDrive/Colab Notebooks/Hekaton/News_Classification/data.zip'

"""## Read Train set"""

train = pd.read_csv('train_data.csv')
train.head()

train.shape

train.info()

train['tags'].astype('str')

train['tags'].unique()

train.isnull().sum()

train[train['tags'].isnull()]

train.loc[11659, 'tags'] = 'سیاسی'
train.loc[12186, 'tags'] = 'اجتماعی'

train[train['tags'] == 'بایگانی']

train['tags'].value_counts().iloc[:10]

train['tags'].value_counts().iloc[10:20]

train['tags'].value_counts().iloc[20:30]

train['tags'].value_counts().iloc[30:40]

train['tags'].value_counts().iloc[40:50]

train['tags'].value_counts().iloc[50:60]

train['tags'].value_counts().iloc[60:70]

train['tags'].value_counts().iloc[70:80]

train['tags'].value_counts().iloc[80:]

"""### Change Tags Into Desired Tags"""

news_tags = ['اجتماعی', 'اقتصادی', 'ایران_استانها', 'بین الملل', 'سیاسی', 'علمی_فرهنگی_ورزشی']

def change_tags(x):
  if x in news_tags:
    return x
  else:
    if x.find('اجتماعی') != -1:
      x = 'اجتماعی'
    elif x.find('اقتصادی') != -1:
      x = 'اقتصادی'
    elif x.find('اقتصاد') != -1:
      x = 'اقتصادی'
    elif x.find('ايران') != -1:
      x = 'ایران_استانها'
    elif x.find('استان ها') != -1:
      x = 'ایران_استانها'
    elif x.find('استانها') != -1:
      x = 'ایران_استانها'
    elif x.find('بین الملل') != -1:
      x = 'بین الملل'
    elif x.find('سیاسی') != -1:
      x = 'سیاسی'
    elif x.find('علمی') != -1:
      x = 'علمی_فرهنگی_ورزشی'
    elif x.find('فرهنگی') != -1:
      x = 'علمی_فرهنگی_ورزشی'
    elif x.find('ورزش') != -1:
      x = 'علمی_فرهنگی_ورزشی'
    elif x.find('ورزشی') != -1:
      x = 'علمی_فرهنگی_ورزشی'
    elif x.find('هنر') != -1:
      x = 'علمی_فرهنگی_ورزشی'
    return x

train['tags'] = train['tags'].apply(change_tags)

train['tags'].unique()

train['tags'].value_counts()

train[train['tags'] == 'آلمان']['description']

train[train['tags'] == 'خبرخوان']['description']

train.loc[train['tags'] == 'آلمان', 'tags'] = 'بین الملل'
train.loc[train['tags'] == 'جهان', 'tags'] = 'بین الملل'
train.loc[train['tags'] == 'ایران', 'tags'] = 'ایران_استانها'
train.loc[train['tags'] == 'خبرخوان', 'tags'] = 'ایران_استانها'
train.loc[train['tags'] == 'دانش و محیط زیست', 'tags'] = 'علمی_فرهنگی_ورزشی'
train.loc[train['tags'] == 'دانش و فناوری', 'tags'] = 'علمی_فرهنگی_ورزشی'

train['tags'].value_counts()

train[train['tags'] == 'بایگانی']['description']

train = train[train['tags'].isin(news_tags)]

train.groupby('tags').count()

"""### Pre-Processing

#### Remove Punctuation
"""

from string import punctuation

punctuation += '؛؟ُ،ًٌٍَُِّْــ'
punctuation

def remove_punctuation(sentence: str) -> str:
    return sentence.translate(str.maketrans('', '', punctuation))

"""#### Remove Persian Stop Words"""

stop = np.load('/content/drive/MyDrive/Colab Notebooks/Hekaton/stopwords.npy')
stop

print(stop[19])

def remove_stopwords(text):
    text = ' '.join([i for i in text.lower().split(' ') if i not in stop])
    return text

"""#### Apply Pre-Processing Part"""

train['title'] = train['title'].apply(remove_stopwords)
train['title'] = train['title'].apply(lambda x:x.replace('\u200c', ''))
train['title'] = train['title'].apply(remove_punctuation)

train['description'] = train['description'].apply(remove_stopwords)
train['description'] = train['description'].apply(lambda x:x.replace('\u200c', ''))
train['description'] = train['description'].apply(remove_punctuation)

train.head()

"""#### Normalize and Stem Words"""

from hazm import Normalizer, Stemmer, Lemmatizer
from nltk.stem.porter import PorterStemmer

normalizer = Normalizer()
lemmatizer = Lemmatizer()

train['title'] = train['title'].apply(normalizer.normalize)
train['title'] = train['title'].apply(lemmatizer.lemmatize)

train['description'] = train['description'].apply(normalizer.normalize)
train['description'] = train['description'].apply(lemmatizer.lemmatize)

"""### TFIDF Vectorized"""

from sklearn.feature_extraction.text import TfidfVectorizer

tv_title = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
title = tv_title.fit_transform(train.loc[:,'title'])

tv_desc = TfidfVectorizer(max_features=15000, ngram_range=(1, 2))
desc = tv_desc.fit_transform(train.loc[:,'description'])

print(f"Title Shape: {title.shape}")
print(f"Description Shape: {desc.shape}")

from scipy.sparse import hstack

X_train = hstack((title, desc)).toarray()
print(X_train.shape)

"""## Read Test set"""

test = pd.read_csv('test_data.csv')
test.head()

test.shape

"""### Pre-Processing for Test set"""

test['title'] = test['title'].apply(remove_stopwords)
test['title'] = test['title'].apply(lambda x:x.replace('\u200c', ''))
test['title'] = test['title'].apply(remove_punctuation)

test['description'] = test['description'].apply(remove_stopwords)
test['description'] = test['description'].apply(lambda x:x.replace('\u200c', ''))
test['description'] = test['description'].apply(remove_punctuation)

test['title'] = test['title'].apply(normalizer.normalize)
test['title'] = test['title'].apply(lemmatizer.lemmatize)

test['description'] = test['description'].apply(normalizer.normalize)
test['description'] = test['description'].apply(lemmatizer.lemmatize)

title = tv_title.transform(test.loc[:,'title'])
desc = tv_desc.transform(test.loc[:,'description'])

X_test = hstack((title, desc)).toarray()
print(X_test.shape)

"""## Change Order of Tags Column"""

from pandas.api.types import CategoricalDtype

tags_type = CategoricalDtype(news_tags, ordered=True)
train['tags'] = train['tags'].astype(tags_type)
y_train = pd.get_dummies(train['tags']).values

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)

pd.get_dummies(train['tags'])

"""## Split Dataset"""

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)

print(y_train.shape)
print(y_val.shape)

del (title, desc,
     train, test)

"""## ANN Model"""

from tensorflow import keras
from keras.layers import Dense, Dropout
from keras.models import Sequential

model = Sequential(
    [
        Dense(units=64, input_dim=X_train.shape[-1], activation="relu", name="input"),
        Dense(units=64, activation="relu", name="layer1"),
        Dropout(0.2),
        Dense(units=64, activation="relu", name="layer2"),
        Dense(units=64, activation="relu", name="layer3"),
        Dropout(0.2),
        Dense(units=6, activation="softmax", name="output")
    ],
    name="News_model"
)
model.summary()

from keras import utils

utils.plot_model(model, show_shapes=True, show_layer_names=False)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=64, epochs=10, verbose=2)

"""### Plot Loss and Accuracy"""

import matplotlib.pyplot as plt

f = plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'],
         label='train accuracy',
         c='orange', ls='-')
plt.plot(history.history['val_accuracy'],
         label='val accuracy',
         c='dodgerblue', ls='-')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.subplot(1,2,2)
plt.plot(history.history['loss'],
         label='train loss',
         c='orange', ls='--')
plt.plot(history.history['val_loss'],
         label='val loss',
         c='dodgerblue', ls='--')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='lower right')
plt.show()

"""### Predict $X_{test}$"""

y_pred = model.predict(X_test)
print(y_pred.shape)

y_pred = np.argmax(y_pred, axis=1)
print(y_pred.shape)

sol = pd.DataFrame({'prediction': y_pred})
sol.head(10)

sol.to_csv('sol.csv', index=False)

